<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="RoboCurate: A neural trajectory generation and filtering framework that increases diversity via controllable video generation and filters low-quality samples.">
  <meta name="keywords" content="RoboCurate, robot learning, data augmentation, video generation, action filtering">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RoboCurate: Harnessing Diversity with Action-Verified Neural Trajectory for Robot Learning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    /* ── RoboCurate custom styles ── */
    .robocurate {
      font-weight: 700;
    }

    /* Video grid */
    .video-grid {
      display: grid;
      grid-template-columns: repeat(3, 1fr);
      gap: 12px;
      max-width: 960px;
      margin: 0 auto;
    }

    .video-grid .video-cell {
      position: relative;
      border-radius: 8px;
      overflow: hidden;
      background: #000;
    }

    .video-grid .video-cell video {
      width: 100%;
      display: block;
      border-radius: 8px;
    }

    .video-grid .video-cell .video-label {
      position: absolute;
      bottom: 0;
      left: 0;
      right: 0;
      background: linear-gradient(transparent, rgba(0,0,0,0.65));
      color: #fff;
      font-size: 0.75rem;
      padding: 16px 8px 6px;
      text-align: center;
      font-family: 'Google Sans', sans-serif;
    }

    .video-grid .video-cell .outcome-badge {
      position: absolute;
      top: 6px;
      right: 6px;
      font-size: 0.65rem;
      font-weight: 600;
      padding: 2px 8px;
      border-radius: 4px;
      font-family: 'Google Sans', sans-serif;
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    .outcome-success { background: rgba(72,199,142,0.90); color: #fff; }
    .outcome-partial { background: rgba(255,183,77,0.90); color: #fff; }
    .outcome-fail    { background: rgba(241,70,104,0.90); color: #fff; }

    /* Column & row headers for the grid */
    .grid-header {
      display: grid;
      grid-template-columns: repeat(3, 1fr);
      gap: 12px;
      max-width: 960px;
      margin: 0 auto 4px;
      text-align: center;
      font-family: 'Google Sans', sans-serif;
      font-weight: 600;
      font-size: 0.95rem;
      color: #363636;
    }

    .grid-header .ours-highlight {
      color: #2E7D32;
    }

    .row-label {
      max-width: 960px;
      margin: 18px auto 6px;
      font-family: 'Google Sans', sans-serif;
      font-size: 1.1rem;
      color: #555;
    }
    .row-label:first-of-type {
      margin-top: 10px;
    }

    /* Result image styling */
    .result-image {
      border-radius: 8px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.10);
    }

    /* Method section numbered steps */
    .method-step {
      margin-bottom: 1.5rem;
    }
    .method-step .step-title {
      font-weight: 700;
      font-family: 'Google Sans', sans-serif;
      font-size: 1.05rem;
      margin-bottom: 0.25rem;
    }
    .method-step .method-figure {
      margin-top: 0.75rem;
      border-radius: 8px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.08);
      width: 100%;
    }

    /* Main figure */
    .main-figure img {
      border-radius: 10px;
      box-shadow: 0 4px 16px rgba(0,0,0,0.08);
    }

    /* Observation paragraph */
    .observation {
      background: #f5f7fa;
      border-left: 4px solid hsl(204, 86%, 53%);
      padding: 1rem 1.25rem;
      border-radius: 0 8px 8px 0;
      margin-top: 1.5rem;
      font-size: 0.95rem;
      color: #4a4a4a;
    }

    /* Action alignment 2-column layout */
    .action-overview-grid {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 20px;
      align-items: stretch;
    }
    .action-overview-grid .action-col {
      display: flex;
      flex-direction: column;
      gap: 12px;
      justify-content: center;
    }
    .action-overview-grid .action-item video {
      width: 100%;
      border-radius: 8px;
      display: block;
    }
    .action-overview-grid .action-item .caption,
    .action-overview-grid .data-col .caption {
      font-family: 'Google Sans', sans-serif;
      font-size: 0.85rem;
      color: #555;
      text-align: center;
      margin-top: 4px;
      white-space: nowrap;
    }
    .action-overview-grid .data-col {
      display: flex;
      flex-direction: column;
      justify-content: center;
    }
    .action-overview-grid .data-col img {
      width: 100%;
      border-radius: 8px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.08);
    }

    @media (max-width: 768px) {
      .action-overview-grid {
        grid-template-columns: 1fr;
      }
    }
  </style>
</head>
<body>


<!-- Hero / Title -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="robocurate">RoboCurate</span>: Harnessing Diversity with Action-Verified Neural Trajectory for Robot Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Seungku Kim<sup>*1</sup>,</span>
            <span class="author-block">
              Suhyeok Jang<sup>*1</sup>,</span>
            <span class="author-block">
              Byungjun Yoon<sup>1</sup>,</span>
            <span class="author-block">
              Dongyoung Kim<sup>1,2</sup>,</span>
            <span class="author-block">
              John Won<sup>1</sup>,</span>
            <span class="author-block">
              Jinwoo Shin<sup>1,2</sup></span>
          </div>

          <div class="is-size-7 publication-authors" style="margin-top:2px; color:#888;">
            <span>*Equal contribution</span>
          </div>

          <div class="is-size-5 publication-authors" style="margin-top:6px;">
            <span class="author-block"><sup>1</sup>KAIST,</span>
            <span class="author-block"><sup>2</sup>RLWRLD</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- arXiv Link (placeholder – TBD) -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<!-- Main Figure -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered main-figure">
      <img src="./static/images/robocurate_main_figure.png"
           alt="RoboCurate overview figure"
           style="width:100%;">
    </div>
  </div>
</section>


<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present <span class="robocurate">RoboCurate</span>, a novel neural trajectory generation framework that increases diversity via controllable video generation and filters low-quality samples by evaluating motion similarity between generated video and simulator replay. Specifically, <span class="robocurate">RoboCurate</span> replays the predicted actions in a simulator and assesses action quality by measuring the consistency of motion between the simulator rollout and the generated video. In addition, we unlock observation diversity beyond the available dataset via image-to-image editing and apply action-preserving video-to-video transfer to further augment appearance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Method -->
<section class="section has-background-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Method</h2>
        <div class="content has-text-justified">

          <div class="method-step">
            <p class="step-title">1.&ensp;Generation Stage</p>
            <p>
              We expand observation diversity with two components: (1) image-to-image (I2I) editing on the initial frame for scene-level variation, and (2) video-to-video (V2V) transfer for appearance diversity while preserving initial motion.
            </p>
            <img class="method-figure" src="./static/images/robocurate_stage1_generation.png"
                 alt="Generation stage overview">
          </div>

          <div class="method-step">
            <p class="step-title">2.&ensp;Filtering Stage</p>
            <p>
              We filter suboptimal synthetic trajectories with inaccurate actions by replaying the predicted actions in a simulator and assessing action quality by measuring the consistency of motion between the simulator rollout and the generated video. We train an attentive probe on top of a frozen video encoder to measure motion similarity between the simulator rollout and the generated video with automatically generated positive and negative samples from real data.
            </p>
            <img class="method-figure" src="./static/images/robocurate_stage2_filtering.png"
                 alt="Filtering stage overview">
          </div>

        </div>

        <!-- Action alignment & data overview 2-column -->
        <div class="action-overview-grid" style="max-width: 100%;">
          <div class="action-col">
            <div class="action-item">
              <video autoplay muted loop playsinline>
                <source src="./videos/good_action_alignment.mp4" type="video/mp4">
              </video>
              <p class="caption">Accurate action: Simulator rollout ≈ Synthetic video</p>
            </div>
            <div class="action-item">
              <video autoplay muted loop playsinline>
                <source src="./videos/bad_action_alignment.mp4" type="video/mp4">
              </video>
              <p class="caption">Inaccurate action: Simulator rollout ≠ Synthetic video</p>
            </div>
          </div>
          <div class="data-col">
            <img src="./static/images/data_overview.png"
                 alt="Examples of positive and negative pairs for attentive probe training">
            <p class="caption">Examples of positive and negative pairs for attentive probe training.</p>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>


<!-- Results: ALLEX Humanoid -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Results</h2>

    <h3 class="title is-4 has-text-centered" style="margin-bottom:0.5rem;">ALLEX Humanoid Robot</h3>

    <!-- Row 1: In-distribution – Pick and Place Can -->
    <p class="row-label"><strong>In-distribution task</strong> &mdash; Pick and Place Can</p>
    <div class="grid-header">
      <span>Real</span>
      <span>Real + DreamGen</span>
      <span class="ours-highlight">Real + <span class="robocurate">RoboCurate</span> (Ours)</span>
    </div>
    <div class="video-grid">
      <div class="video-cell">
        <video autoplay muted loop playsinline>
          <source src="./videos/real_demo_pnp_can_fail.mp4" type="video/mp4">
        </video>
        <span class="outcome-badge outcome-fail">Fail</span>
      </div>
      <div class="video-cell">
        <video autoplay muted loop playsinline>
          <source src="./videos/neural_demo_pnp_can_success.mp4" type="video/mp4">
        </video>
        <span class="outcome-badge outcome-success">Success</span>
      </div>
      <div class="video-cell">
        <video autoplay muted loop playsinline>
          <source src="./videos/ours_demo_pnp_can_success.mp4" type="video/mp4">
        </video>
        <span class="outcome-badge outcome-success">Success</span>
      </div>
    </div>

    <!-- Row 2: OOD Novel Object – Pick and Place Cup -->
    <p class="row-label"><strong>Out-of-distribution task (Novel Object)</strong> &mdash; Pick and Place Cup</p>
    <div class="grid-header">
      <span>Real</span>
      <span>Real + DreamGen</span>
      <span class="ours-highlight">Real + <span class="robocurate">RoboCurate</span> (Ours)</span>
    </div>
    <div class="video-grid">
      <div class="video-cell">
        <video autoplay muted loop playsinline>
          <source src="./videos/real_demo_pnp_cup_fail.mp4" type="video/mp4">
        </video>
        <span class="outcome-badge outcome-fail">Fail</span>
      </div>
      <div class="video-cell">
        <video autoplay muted loop playsinline>
          <source src="./videos/neural_demo_pnp_cup_partial.mp4" type="video/mp4">
        </video>
        <span class="outcome-badge outcome-partial">Partial</span>
      </div>
      <div class="video-cell">
        <video autoplay muted loop playsinline>
          <source src="./videos/ours_demo_pnp_cup_success.mp4" type="video/mp4">
        </video>
        <span class="outcome-badge outcome-success">Success</span>
      </div>
    </div>

    <!-- Row 3: OOD Novel Behavior – Pour Can -->
    <p class="row-label"><strong>Out-of-distribution task (Novel Behavior)</strong> &mdash; Pour Can</p>
    <div class="grid-header">
      <span>Real</span>
      <span>Real + DreamGen</span>
      <span class="ours-highlight">Real + <span class="robocurate">RoboCurate</span> (Ours)</span>
    </div>
    <div class="video-grid">
      <div class="video-cell">
        <video autoplay muted loop playsinline>
          <source src="./videos/real_demo_pour_fail.mp4" type="video/mp4">
        </video>
        <span class="outcome-badge outcome-fail">Fail</span>
      </div>
      <div class="video-cell">
        <video autoplay muted loop playsinline>
          <source src="./videos/neural_demo_pour_fail.mp4" type="video/mp4">
        </video>
        <span class="outcome-badge outcome-fail">Fail</span>
      </div>
      <div class="video-cell">
        <video autoplay muted loop playsinline>
          <source src="./videos/ours_demo_pour_success.mp4" type="video/mp4">
        </video>
        <span class="outcome-badge outcome-success">Success</span>
      </div>
    </div>
  </div>
</section>


<!-- Results: GR-1 Tabletop & DexMimicGen -->
<section class="section has-background-light">
  <div class="container is-max-desktop">

    <!-- GR-1 Tabletop -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">GR-1 Tabletop</h3>
        <p class="content has-text-justified" style="margin-bottom:1rem;">
          We report the average success rate (%) over 50 trials across 24 tasks (18 rearrangement and 6 articulated).
        </p>
        <img class="result-image" src="./static/images/gr1_tabletop_result.png"
             alt="GR-1 Tabletop results table"
             style="width:100%;">
      </div>
    </div>

    <br>

    <!-- DexMimicGen -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">DexMimicGen</h3>
        <p class="content has-text-justified" style="margin-bottom:1rem;">
          We report the average success rate (%) over 50 trials across 6 tasks (3 GR-1 Humanoid and 3 Bimanual Panda Arms with Dexterous Hands), trained with 100 demonstrations per task.
        </p>
        <img class="result-image" src="./static/images/dexmimicgen_result.png"
             alt="DexMimicGen results table"
             style="width:100%;">
      </div>
    </div>

    <!-- Observation -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="observation">
          We observe that our visual augmentation pipeline (e.g., I2I editing and V2V transfer) substantially improves downstream task performance. Moreover, our action-level filtering is effective for curating neural trajectories and further enhances VLA performance.
        </div>
      </div>
    </div>

  </div>
</section>


<!-- BibTeX (placeholder) -->
<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{kim2025robocurate,
  author    = {Kim, Seungku and Jang, Suhyeok and Yoon, Byungjun and Kim, Dongyoung and Won, John and Shin, Jinwoo},
  title     = {RoboCurate: Neural Trajectory Generation and Filtering},
  year      = {2025},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content" style="text-align:center;">
          <p>
            This website is based on the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page template, licensed under a
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
